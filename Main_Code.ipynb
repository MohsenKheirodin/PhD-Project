{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9bf85d-9483-4eca-ae7b-dd3f2672c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Tasks\n",
    "# 5. adding price itself to the model or fractional differentiation\n",
    "# 25. Checking 30 lag input\n",
    "# 13. Focus on important points where a cross happens.\n",
    "# 3. Checking the improvement that can be achieved by three border labeling for long term modeling that fundamental data are working better\n",
    "# 15. Try to predict extreme events or their behavior after starting a trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170d74d-45a8-4d62-b262-c6b1ab6aa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From Now on\n",
    "\n",
    "# 1. Do regime seperation models again, and do the same job for fundamental features as well as size\n",
    "# 2. Reduce the dimension and check if this work solve the problem of local vs global models accuracy\n",
    "# 3. Add concept drift module to current online learning code\n",
    "# 4. Work on Regime Problem\n",
    "# 5. Check the Midas Model for monthly prediction by Short term information\n",
    "# 6. Add Monthly Info\n",
    "\n",
    "# During the week\n",
    "# 1. Add CNN model and check other models\n",
    "# 2. Add Shap Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11792c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  preparing_data import *\n",
    "from  preparing_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155673e-40c4-458d-a242-780ab218f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import Preparing_Data\n",
    "import Learning_Models\n",
    "\n",
    "%run ./Learning_Models.ipynb\n",
    "%run ./Preparing_Data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3414d0-a59b-411d-9453-80ed8d22cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short_term_list   = ['return_close', 'diff_close_vwap', 'during_ret_close', 'open_return', 'up_shadow', 'down_shadow', 'body', 'rizesh_w', 'khizesh_w', 'body_w', 'profit_w',\n",
    "                     'upper_bound','lower_bound','break_borders']\n",
    "long_term_list    = ['rizesh_m', 'khizesh_m', 'body_m', 'profit_m', 'rizesh_y', 'khizesh_y', 'body_y', 'profit_y']\n",
    "activity_list     = ['money_in', 'sarane_ind', 'money_in_w', 'money_in_m','ave_val', 'ave_val_w']\n",
    "pivot_points      = ['valy4', 'pik4', 'valy3', 'pik3', 'valy2', 'pik2', 'valy1', 'pik1']\n",
    "regime_list       = ['std', 'body_rolling', 'regime','regime_market','currnt_reg_days','currnt_reg_days_market']\n",
    "# general_info_list = ['symbol', 'status', 'que', 'prc_mdf', 'market_cap','size','industry']\n",
    "general_info_list = ['symbol', 'status', 'que', 'prc_mdf', 'market_cap']\n",
    "technical_list    = ['trend_macd','trend_macd_signal','trend_macd_diff','momentum_rsi','volume_mfi','momentum_roc']\n",
    "price_list        = ['adj_close', 'adj_vwap','index_vwap']\n",
    "\n",
    "fundamental_eng  = ['net_margin_short_term', 'gross_margin_short_term','net_margin_long_term', 'gross_margin_long_term', 'net_growth_ma','gross_growth_ma', \n",
    "                    'sales_growth_ma','E/P','S/P'] \n",
    "fundamental_raw  = ['Sales', 'Cost of Sales', 'Gross Profit (Loss)','Sales, General and Administrative Expense','Other Operating Revenue (Expenses)', \n",
    "                    'Operating Profit (Loss)','Finance Expense', 'Income Tax Expense', 'Net Income (Loss)']\n",
    "fundmental_ratio = ['change_sales','change_gross','change_operating','change_net','gross_ratio_change','oprtg_ratio_change','net_ratio_change',\n",
    "                    'gross_margin','operating_margin','net_margin']\n",
    "\n",
    "fund_features_for_z_scoring = ['net_margin_short_term','gross_margin_short_term','net_margin_long_term','gross_margin_long_term','net_growth_ma','gross_growth_ma',\n",
    "                               'sales_growth_ma','change_sales', 'change_gross', 'change_operating', 'change_net','gross_ratio_change', 'oprtg_ratio_change', \n",
    "                               'net_ratio_change','gross_margin', 'operating_margin', 'net_margin']\n",
    "fundamental_z_scores = []\n",
    "for item in fund_features_for_z_scoring:\n",
    "    fundamental_z_scores.append(item+'_z_score')\n",
    "\n",
    "fundamental_list = fundamental_eng + fundamental_raw + fundmental_ratio + fundamental_z_scores\n",
    "\n",
    "lag_days = [1,3,5,10,15,20,25,30,40,50,60]\n",
    "lag_labels = [1, 5]\n",
    "\n",
    "features_dictionary = {'SHORT_TERM':short_term_list, 'LONG_TERM':long_term_list, 'ACTIVITY':activity_list, 'PIVOT':pivot_points, 'REGIME':regime_list, 'GENERAL':general_info_list, \n",
    "                       'TECHNICAL':technical_list, 'PRICE': price_list, 'FUND': fundamental_list}\n",
    "\n",
    "with open('features_dictionary.pkl', 'wb') as file:\n",
    "    pickle.dump(features_dictionary, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b2865-c2f6-43b6-bfc4-8c4ff7fd8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # zero Step: Get Raw Data\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# Get_Raw_Data()\n",
    "# warnings.simplefilter('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ecc1f-ae32-47ba-a37e-976ec0cce757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First Step: Calculate the features and delete weird dates\n",
    "\n",
    "# first_time = False\n",
    "\n",
    "# if first_time:\n",
    "#     modify = {'Close_days': True, 'ques' : True, 'pivots' : True, 'regime' : True, 'tse_info' : True, 'technical' : True} \n",
    "#     price = pd.read_parquet('price_raw_data.parquet')\n",
    "# else:\n",
    "#     modify = {'Close_days': False, 'ques' : False, 'pivots' : False, 'regime' : True, 'tse_info' : False, 'technical' : False} \n",
    "#     price = pd.read_parquet('price_calculated_features.parquet')\n",
    "#     columns_to_delete = ['regime','regime_market']\n",
    "#     price = price.drop(columns = columns_to_delete)\n",
    "\n",
    "# symbols_1 = price['symbol'].unique().tolist()\n",
    "# symbols_1 = [item for item in symbols_1 if 'تسه' not in item and 'حذف' not in item and 'پذيره' not in item]\n",
    "\n",
    "# df_index_info = {'Regime' : True, 'Value' : False, 'Fundamental' : False, 'Oil': False, 'Risk_free' : False, 'Dollar' : False}\n",
    "# df_index, _, _ = Create_Index(df_index_info)\n",
    "\n",
    "# df_price_daily_feat = pd.DataFrame()\n",
    "# counter = 0\n",
    "# for symbol in symbols_1[0:]:   \n",
    "#     try:\n",
    "        \n",
    "#         price_info = price[price['symbol'] == symbol]\n",
    "#         price_stock = calculate_price_features(price = price_info, col = 'adj_close', df_index = df_index, modify = modify)  \n",
    "#         df_price_daily_feat = pd.concat([df_price_daily_feat, price_stock], axis = 0)        \n",
    "        \n",
    "#         counter = counter + 1\n",
    "#         print(str(counter) + ' : ' + str(symbol))\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print('Error in ' + str(symbol) + ' : ')\n",
    "#         print(e)\n",
    "\n",
    "# list_initial = pd.read_excel('get (1).xlsx')\n",
    "# list_initial['symbol'] = list_initial['symbol'].apply(lambda x: x.replace('ي','ی').replace('ك','ک'))\n",
    "# df_price_daily_feat, list = categorizing_Based_on_Indsry_size(list = list_initial, price = df_price_daily_feat)\n",
    "# df_price_daily_feat.to_parquet('price_calculated_features.parquet', index=True)\n",
    "# list.to_parquet('list_stock.parquet', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3db27-aba0-44dc-8860-7d5121b42241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: part 2\n",
    "\n",
    "df_price_daily_feat = pd.read_parquet('price_calculated_features.parquet')\n",
    "\n",
    "list_price_features_for_z_scoring = ['return_close', 'diff_close_vwap', 'during_ret_close', 'open_return', 'up_shadow', 'down_shadow', 'body', 'rizesh_w', 'khizesh_w', 'body_w', 'profit_w', \n",
    "                                     'rizesh_m', 'khizesh_m', 'body_m', 'profit_m', 'rizesh_y', 'khizesh_y', 'body_y', 'profit_y', 'money_in', 'sarane_ind', 'money_in_w', 'money_in_m','ave_val', 'ave_val_w', \n",
    "                                     'std', 'body_rolling', 'regime','regime_market','currnt_reg_days','currnt_reg_days_market', 'trend_macd','trend_macd_signal','trend_macd_diff','momentum_rsi','volume_mfi','momentum_roc']\n",
    "list_z_price_features = []\n",
    "for item in list_price_features_for_z_scoring:\n",
    "    list_z_price_features.append(item+'_z_score')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "all_dates = df_price_daily_feat.index.unique().tolist()\n",
    "df_price_daily_feat_z_score = pd.DataFrame()\n",
    "\n",
    "for date in tqdm(all_dates[0:], desc=\"Processing symbols\"):\n",
    "    df_price_daily_stock = df_price_daily_feat.loc[df_price_daily_feat.index == date]\n",
    "    for item in list_price_features_for_z_scoring:\n",
    "        df_price_daily_stock[item+'_z_score'] = (df_price_daily_stock[item] - df_price_daily_stock[item].mean()) / df_price_daily_stock[item].std() \n",
    "    df_price_daily_feat_z_score = pd.concat([df_price_daily_feat_z_score, df_price_daily_stock])\n",
    "    \n",
    "df_price_daily_feat_z_score.to_parquet('price_calculated_features_z_score.parquet', index=True)\n",
    "warnings.simplefilter('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096940b0-3094-4830-b15e-9d41eeba5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_daily_feat_z_score[list_price_features_for_z_scoring+list_z_price_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584e776-d87f-4d15-b35e-dcbb27bccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second step: Calculating the Outputs and normalizing\n",
    "\n",
    "# Lag_days = [1,3,5,10,15,20,25,30,40,50,60]\n",
    "\n",
    "# price = pd.read_parquet('price_calculated_features.parquet')\n",
    "# symbols = price['symbol'].unique().tolist()\n",
    "\n",
    "# df_index = pd.read_parquet('index_raw_data.parquet')\n",
    "\n",
    "# df_price_daily_ready = pd.DataFrame()\n",
    "# counter = 0\n",
    "# for symbol in symbols:   \n",
    "#     try:\n",
    "#         print(symbol)\n",
    "#         price_info = price[price['symbol'] == symbol]\n",
    "#         price_stock = calculate_price_output_and_preproccessing(price = price_info, lag_days = Lag_days, col = 'adj_close', outlier = 0, normalization = False) \n",
    "#         df_price_daily_ready = pd.concat([df_price_daily_ready, price_stock], axis = 0)  \n",
    "        \n",
    "#         counter = counter + 1\n",
    "#         print(str(counter) + ' : ' + str(symbol))\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print('Error in ' + str(symbol) + ' : ')\n",
    "#         print(e)\n",
    "\n",
    "# df_price_daily_ready.to_parquet('price_ready_for_Labeling.parquet', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5931d-f701-428c-ba3c-e3920a9e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3th step: Adding Fundamental features\n",
    "\n",
    "# price  = pd.read_parquet('price_raw_data.parquet')\n",
    "# symbols = price['symbol'].unique().tolist()\n",
    "# symbols = [item for item in symbols if 'تسه' not in item and 'حذف' not in item and 'پذيره' not in item]\n",
    "# income = pd.read_parquet('income_raw_data.parquet')\n",
    "\n",
    "# df_price_whole = pd.DataFrame()\n",
    "# counter_un = 0\n",
    "\n",
    "# for symbol in tqdm(symbols[0:], desc=\"Processing symbols\"):   \n",
    "#     try:        \n",
    "#         price_stock = price[price['symbol'] == symbol]\n",
    "#         df = income[income['symbol'] == symbol]\n",
    "        \n",
    "#         _ , df_price = Calculate_fundamental_state(price_stock, df.reset_index())\n",
    "#         df_price_whole = pd.concat([df_price_whole, df_price], axis = 0)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         counter_un = counter_un + 1\n",
    "        \n",
    "# df_price_whole.to_parquet('price_with_fundamental_features.parquet', index=True)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ffd1c-c87d-4190-b8f8-1ef8fdf5e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3rd step: 2nd phase\n",
    "\n",
    "# df_price_whole = pd.read_parquet('price_with_fundamental_features.parquet') \n",
    "# df_price_whole = df_price_whole.sort_values('period_ending_date')\n",
    "# indexes = df_price_whole['period_ending_date'].unique()\n",
    "# stocks = df_price_whole['symbol'].unique().tolist()\n",
    "\n",
    "# valid_dates = []\n",
    "# for date in indexes:\n",
    "#     price_index = df_price_whole[df_price_whole['period_ending_date'] == date]\n",
    "#     if price_index['symbol'].unique().shape[0]>100:\n",
    "#         valid_dates.append(date)\n",
    "\n",
    "# list_fund_features_for_z_scoring = ['net_margin_short_term','gross_margin_short_term','net_margin_long_term','gross_margin_long_term','net_growth_ma','gross_growth_ma','sales_growth_ma',\n",
    "#                                     'change_sales', 'change_gross', 'change_operating', 'change_net','gross_ratio_change', 'oprtg_ratio_change', 'net_ratio_change','gross_margin', 'operating_margin', 'net_margin']\n",
    "# list_z_fund_features = []\n",
    "# for item in list_fund_features_for_z_scoring:\n",
    "#     list_z_fund_features.append(item+'_z_score')\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# represent_whole = df_price_whole[df_price_whole['period_ending_date'].isin(valid_dates)]\n",
    "# represent_level_2 = pd.DataFrame()\n",
    "\n",
    "# for date in tqdm(valid_dates[0:], desc=\"Processing symbols\"):\n",
    "\n",
    "#     stocks = represent_whole.loc[represent_whole['period_ending_date'] == date,'symbol'].unique().tolist()    \n",
    "#     represent_potential = represent_whole[represent_whole['period_ending_date'] == date]\n",
    "#     all_dates = represent_potential.sort_index().index.unique()\n",
    "#     represent_total_unique = represent_potential[represent_potential['publish'] == 2]\n",
    "    \n",
    "#     represent_level_1 = pd.DataFrame()\n",
    "#     stocks_added = []\n",
    "#     for single_date in all_dates[0:]:\n",
    "#         stock_remained = [item for item in stocks if item not in stocks_added]\n",
    "#         selected_stocks = represent_total_unique.loc[represent_total_unique['publish_date'] < single_date, 'symbol'].unique().tolist() \n",
    "#         stocks_to_be_added = [item for item in stock_remained if item in selected_stocks]\n",
    "#         stocks_added.extend(stocks_to_be_added)\n",
    "        \n",
    "#         represent = represent_total_unique[represent_total_unique['symbol'].isin(stocks_added)]        \n",
    "#         represent['confidence'] = len(stocks_added)/len(stocks)\n",
    "        \n",
    "#         if represent.shape[0]>0:         \n",
    "#             for item in list_fund_features_for_z_scoring:\n",
    "#                 represent[item+'_z_score'] = (represent[item] - represent[item].mean()) / represent[item].std() \n",
    "#         represent = represent.reset_index()[['symbol','confidence']+list_z_fund_features]\n",
    "#         stocks_avilable = represent_potential.reset_index()[represent_potential.index == single_date]\n",
    "#         stocks_avilable = stocks_avilable.merge(represent, on = 'symbol', how = 'left').set_index('date').sort_index()\n",
    "#         represent_level_1 = pd.concat([represent_level_1, stocks_avilable])\n",
    "    \n",
    "#     represent_level_2 = pd.concat([represent_level_2, represent_level_1])\n",
    "# represent_level_2.to_parquet('price_with_fundamental_features_added.parquet', index=True)\n",
    "# warnings.simplefilter('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba50197-53ec-46e5-aad6-c7ab9f57e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 4th Step: Add Labels for Classification\n",
    "# update = True\n",
    "# Lag_days_with_label = [1, 5, 10, 20]\n",
    "\n",
    "# price = pd.read_parquet('price_ready_for_Labeling.parquet')\n",
    "# delete_days1 = pd.date_range(start='2021-04-11', end='2021-05-10')\n",
    "# price = price[~price.index.isin(delete_days1)]\n",
    "\n",
    "# delete_days2 = pd.date_range(start='2024-07-08', end='2024-08-15')\n",
    "# price = price[~price.index.isin(delete_days2)]\n",
    "\n",
    "# funds = pd.read_parquet('price_with_fundamental_features.parquet')\n",
    "# fund_symbols = funds['symbol'].unique().tolist()\n",
    "# price = price[price['symbol'].isin(fund_symbols)]\n",
    "# del funds, fund_symbols\n",
    "\n",
    "# cols = ['symbol']\n",
    "# for lag in Lag_days_with_label:\n",
    "#     cols.append('ret_abs_' + str(lag) + '_days')\n",
    "\n",
    "# # To reduce the number of columns and make calculations faster\n",
    "# price_limited = price[cols]\n",
    "\n",
    "# if update:\n",
    "#     price_info_labeled = pd.read_parquet('price_ready_after_Labeling.parquet')\n",
    "\n",
    "# for lag in Lag_days_with_label:\n",
    "#     lag_title = 'label_multi_classes_' + str(lag) + '_days'\n",
    "#     if lag_title in price_info_labeled.columns.tolist():\n",
    "#         print ('Lag ' + str(lag) + ' already exists.')\n",
    "#     else:\n",
    "#         price_info_labeled = Relative_Labeling_Data(price_info_whole = price_limited, days = lag)\n",
    "#         price_limited = price_info_labeled\n",
    "    \n",
    "# cols.remove('symbol')\n",
    "# price_info_labeled = price_info_labeled.drop(columns = cols)\n",
    "\n",
    "# price_info_labeled.to_parquet('price_ready_after_Labeling.parquet', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18fd72-a467-401a-8e00-f3bda88895a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 5th step: Read the Data and Merge it\n",
    "\n",
    "# lag_model = {'return_close': 3, 'return_vwap'      : 3, 'during_ret_close': 3, 'during_ret_vwap': 3, 'open_return'    : 3, 'up_shadow'   : 3, 'down_shadow' : 3, 'body'       : 3, \n",
    "#             'diff_ShT_LT'  : 3, 'diff_ShT'         : 3, 'diff_LT'         : 3,\n",
    "#             'rizesh_w'     : 1, 'khizesh_w'        : 1, 'body_w'          : 1, 'rizesh_m'       : 1, 'khizesh_m'      : 1, 'body_m'      : 1, 'rizesh_y'    : 1, 'khizesh_y'  : 1, 'body_y': 1, \n",
    "#             'money_in'     : 3, 'sarane_ind'       : 3, 'money_in_w'      : 1, 'money_in_m'     : 1,\n",
    "#             'ave_val'      : 1, 'ave_val_w'        : 1, \n",
    "#             'std'          : 0, 'body_rolling'     : 0, 'regime'          : 1, 'regime_market'  : 1,\n",
    "#             'piv1'         : 1, 'piv2'             : 1, 'piv3'            : 1, 'piv4'           : 1, 'piv5'           : 1, 'piv6'        : 1, 'piv7'        : 1, 'piv8'       : 1,\n",
    "#             'trend_macd'   : 1, 'trend_macd_signal': 1, 'trend_macd_diff' : 1, 'trend_ema_fast' : 1, 'trend_ema_slow' : 1,'momentum_rsi' : 1,'volume_mfi'  : 1,'momentum_roc' : 1,\n",
    "#             'size'         : 1, 'industry'         : 1, 'period_ending_date' : 1}\n",
    "\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# P_info, feat_created   = Merge_and_Clean_DFs(lag_model = lag_model, time_series = False)\n",
    "# # P_info_s = Merge_and_Clean_DFs(lag_model = lag_model, time_series = True)\n",
    "\n",
    "# warnings.simplefilter('default')\n",
    "\n",
    "# P_info.loc[:,~P_info.columns.duplicated()].to_parquet('P_info.parquet', index=True)    \n",
    "# # P_info_s.loc[:,~P_info.columns.duplicated()].to_parquet('P_info_s.parquet', index=True)\n",
    "\n",
    "# with open('feat_created.pkl', 'wb') as file:\n",
    "#     pickle.dump(feat_created, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019cd36-80c4-45ce-9ca2-9919bfce9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th step: prepare X_train, X_test, y_train, y_test\n",
    "\n",
    "P_info = pd.read_parquet('P_info.parquet')\n",
    "with open('feat_created.pkl', 'rb') as file:\n",
    "        feat_created = pickle.load(file)\n",
    "    \n",
    "lag_days = [1,3,5,10,15,20,25,30,40,50,60]\n",
    "lag_labels = [1, 5, 10, 20]\n",
    "price_desired_features = {'price_Features_Short_term' : True, 'price_Features_Long_term': True, 'technical_features': True, 'pivot_features' : True, 'activity_features': True, \n",
    "                          'regime_features' : True, 'fundamental_features': False, 'general_info': False}\n",
    "\n",
    "X_train, X_test, y_train, y_test, seperation_date = Making_XY_TrainTest(feat_dict = price_desired_features, feat_list_created = feat_created, price_info = P_info, lag_days = lag_days, lag_labels = lag_labels, \n",
    "                                                                        time_series = False, split = 0.8, QC_check = False)\n",
    "\n",
    "X_train = X_train.loc[:,~X_train.columns.duplicated()]\n",
    "y_train = y_train.loc[:,~y_train.columns.duplicated()]\n",
    "X_test  = X_test.loc[:,~X_test.columns.duplicated()]\n",
    "y_test  = y_test.loc[:,~y_test.columns.duplicated()]\n",
    "\n",
    "del P_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b8b4e-2510-433f-ba47-8d00035c9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Extra Filter:\n",
    "\n",
    "# # Filter 1\n",
    "\n",
    "# y_train['publish'] = X_train['publish']\n",
    "# y_test['publish'] = X_test['publish']\n",
    "\n",
    "# X_train = X_train[X_train['publish'] == 2]\n",
    "# y_train = y_train[y_train['publish'] == 2]\n",
    "# X_test  = X_test[X_test['publish'] == 2]\n",
    "# y_test  = y_test[y_test['publish'] == 2]\n",
    "\n",
    "# # Filter 2\n",
    "\n",
    "# dates = []\n",
    "# y_train['delete'] = 1\n",
    "# for i in y_train['period_ending_date'].unique():\n",
    "#     if y_train[y_train['period_ending_date'] == i].shape[0] > 50:\n",
    "#         dates.append(i)\n",
    "#         y_train.loc[y_train['period_ending_date'] == i, 'delete'] = 0\n",
    "        \n",
    "# X_train['delete'] = y_train['delete']\n",
    "# X_train = X_train[X_train['delete'] == 0]\n",
    "# y_train = y_train[y_train['delete'] == 0]\n",
    "\n",
    "# y_test['delete'] = 1\n",
    "# for i in y_test['period_ending_date'].unique():\n",
    "#     if y_test[y_test['period_ending_date'] == i].shape[0] > 50:\n",
    "#         dates.append(i)\n",
    "#         y_test.loc[y_test['period_ending_date'] == i, 'delete'] = 0\n",
    "        \n",
    "# X_test['delete'] = y_test['delete']\n",
    "# X_test = X_test[X_test['delete'] == 0]\n",
    "# y_test = y_test[y_test['delete'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71715aa3-1de6-482e-9160-cdcabdfbeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "y_pred_dt_binary, y_proba_dt_binary, importance_dt_binary, dt_model_binary = Decision_tree(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'], max_depth = 3)\n",
    "# y_pred_dt_multi,  y_proba_dt_multi , importance_dt_multi , dt_model_multi  = Decision_tree(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label_four'], max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75592868-19d3-4a1b-9d39-3c56cb534bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XGBoost Model\n",
    "Weight_out = np.array([\n",
    "    [1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "    ])\n",
    "\n",
    "# y_train['weight'] = 1\n",
    "# regime_zero = y_train.loc[(y_train['regime_market_L1'] == 0)].shape[0]\n",
    "# regime_one  = y_train.loc[(y_train['regime_market_L1'] == 1)].shape[0]\n",
    "# y_train.loc[y_train['regime_L1'] == 1,'weight'] = regime_zero / regime_one\n",
    "# weights_input = y_train['weight'].tolist()\n",
    "\n",
    "# y_pred_xgb_binary, y_pred_xgb_binary, importance_XGB_binary, xgb_model_binary = XGB_RFE(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'], mode = 'binary', num_feat = 3)\n",
    "\n",
    "# y_pred_xgb_binary,   y_proba_xgb_binary,  importance_xgb_binary, _,  xgb_model_binary  = XGB(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'], is_binary = True)\n",
    "y_pred_xgb_multi,    y_proba_xgb_multi, _ ,   importance_xgb_multi, xgb_model_multi, _   = XGB(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_multi_classes_1_days'] - 1, y_test['label_multi_classes_1_days'] - 1, is_binary = False, \n",
    "                                                                                     Weight_out = None, Weight_input=None, Bagging=False, num_boost=50, alpha=0, lambda_=0)\n",
    "\n",
    "y_pred_xgb_binary,    y_proba_xgb_binary, _ ,  importance_xgb_multi, xgb_model_multi, _   = XGB(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_binary_abs_3_days'], y_test['label_binary_abs_3_days'], is_binary = True, \n",
    "                                                                                     Weight_out = None, Weight_input=None, Bagging=False, num_boost=50, alpha=0, lambda_=0)\n",
    "\n",
    "# y_test_xgb_regrs, _ , y_train_xgb_regrs , importance_xgb_regrs, xgb_model_regrs   = XGB(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four_LT'], y_test['label_four_LT'], is_binary = False, mode = 'regression',\n",
    "#                                                                                     Bagging=False, num_boost=50, alpha=0, lambda_=0)\n",
    "\n",
    "# xgb_model_regrs.save_model('global_xgb_multi_model.json')\n",
    "# y_pred_xgb_multi_tuned, y_proba_xgb_multi_tuned = xgb_fine_tune_stock_level(X_train, X_test, y_train, y_test, model_name = 'global_xgb_multi_model.json',n_estimators=30, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffbf6b-7e78-4304-8e55-5337e7903fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754ff4b-6048-4908-aa7e-b98e6dfd971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Local and Global models\n",
    "# 1. Local Model\n",
    "\n",
    "X_test = X_test[X_test['symbol'].isin(X_train['symbol'])]\n",
    "y_test = y_test[y_test['symbol'].isin(X_train['symbol'])]\n",
    "\n",
    "X_train = X_train[X_train['symbol'].isin(X_test['symbol'])]\n",
    "y_train = y_train[y_train['symbol'].isin(X_test['symbol'])]\n",
    "\n",
    "black_list = []\n",
    "out_sym = []\n",
    "out_acc_train, out_acc_test = [], [] \n",
    "y_proba_df_total = pd.DataFrame()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for sym in tqdm(X_train['symbol'].unique().tolist()[:], desc=\"Processing symbols\"):\n",
    "    \n",
    "    X_train_sym = X_train[X_train['symbol'] == sym]\n",
    "    y_train_sym = y_train[y_train['symbol'] == sym]\n",
    "    X_test_sym  = X_test[X_test['symbol'] == sym]\n",
    "    y_test_sym  = y_test[y_test['symbol'] == sym]\n",
    "\n",
    "    if X_train_sym.shape[0] > 100:\n",
    "                \n",
    "        y_pred_xgb_multi,    y_proba_xgb_multi, _ ,   importance_xgb_multi, xgb_model_multi, acc   = XGB(X_train_sym.iloc[:,1:], X_test_sym.iloc[:,1:], y_train_sym['label_multi_classes_1_days'] - 1, y_test_sym['label_multi_classes_1_days'] - 1, is_binary = False, \n",
    "                                                                                         Weight_out = None, Weight_input=None, Bagging=False, num_boost=50, alpha=0, lambda_=0, show_result = False)\n",
    "                \n",
    "        out_sym.append(sym)\n",
    "        out_acc_train.append(acc[0])\n",
    "        out_acc_test.append(acc[1])\n",
    "        \n",
    "        y_proba_df = pd.DataFrame(y_pred_xgb_multi, columns=['label_predicted'], index=y_test_sym.index) \n",
    "        y_proba_df['symbol'] = y_test_sym['symbol']\n",
    "        y_proba_df['que'] = y_test_sym['que']  \n",
    "        y_proba_df['label_happened'] = y_test_sym['label_multi_classes_1_days']\n",
    "        y_proba_df['label_predicted'] = y_proba_df['label_predicted'] + 1\n",
    "        y_proba_df['return'] = y_test_sym['ret_abs_1_days'] \n",
    "    \n",
    "        y_proba_df_total = pd.concat([y_proba_df_total, y_proba_df])\n",
    "\n",
    "    else:        \n",
    "        black_list.append(sym)\n",
    "        \n",
    "Single_Input_Result = pd.DataFrame({'symbol': out_sym, 'Acc_Train': out_acc_train, 'Acc_Test': out_acc_test}).set_index('symbol')\n",
    "\n",
    "warnings.simplefilter('default')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65a502-4162-44f2-a0e5-20a5e7ba6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Global Model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train = X_train[~X_train['symbol'].isin(black_list)]\n",
    "y_train = y_train[~y_train['symbol'].isin(black_list)]\n",
    "\n",
    "X_test = X_test[~X_test['symbol'].isin(black_list)]\n",
    "y_test = y_test[~y_test['symbol'].isin(black_list)]\n",
    "\n",
    "y_pred_xgb_multi, y_proba_xgb_multi, y_pred_xgb_multi_train , _ , importance_xgb_multi, xgb_model_multi, acc = XGB(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_multi_classes_1_days'] - 1, y_test['label_multi_classes_1_days'] - 1, is_binary = False, \n",
    "                                                                                     Weight_out = None, Weight_input=None, Bagging=False, num_boost=50, alpha=0, lambda_=0, show_result = False)\n",
    "\n",
    "y_proba_df_test  = pd.DataFrame(y_pred_xgb_multi, columns=['Class 1'], index=y_test.index) \n",
    "y_proba_df_train = pd.DataFrame(y_pred_xgb_multi_train, columns=['Class 1'], index=y_train.index) \n",
    "\n",
    "_ , y_proba_df_test  = Analyze_output(y_proba_df = y_proba_df_test, y_test = y_test, remove_ques = True, pred_is_binary = False, actual_is_binary = False, pred_label_based_on = 'Class 1', \n",
    "                                            actual_label_based_on = 'label_multi_classes_1_days', return_based_on = 'ret_abs_1_days')\n",
    "_ , y_proba_df_train = Analyze_output(y_proba_df = y_proba_df_train, y_test = y_train, remove_ques = True, pred_is_binary = False, actual_is_binary = False, pred_label_based_on = 'Class 1', \n",
    "                                            actual_label_based_on = 'label_multi_classes_1_days', return_based_on = 'ret_abs_1_days')\n",
    "\n",
    "\n",
    "out_sym = []\n",
    "out_acc_train, out_acc_test = [], [] \n",
    "\n",
    "for sym in tqdm(X_train['symbol'].unique().tolist()[:], desc=\"Processing symbols\"):\n",
    "\n",
    "    y_proba_df_train_stock = y_proba_df_train[y_proba_df_train['symbol'] == sym]\n",
    "    y_proba_df_test_stock  = y_proba_df_test[y_proba_df_test['symbol'] == sym]\n",
    "    \n",
    "    accuracy_train = accuracy_score(y_proba_df_train_stock['label_happened'], y_proba_df_train_stock['label_predicted'])\n",
    "    accuracy_test  = accuracy_score(y_proba_df_test_stock['label_happened'],  y_proba_df_test_stock['label_predicted'])\n",
    "    out_sym.append(sym)\n",
    "    out_acc_train.append(accuracy_train)\n",
    "    out_acc_test.append(accuracy_test)\n",
    "    \n",
    "warnings.simplefilter('default')  \n",
    "\n",
    "Global_Input_Result = pd.DataFrame({'symbol': out_sym, 'Acc_Train_global': out_acc_train, 'Acc_Test_global': out_acc_test}).set_index('symbol')    \n",
    "Comparison_single_global_models = Single_Input_Result.reset_index().merge(Global_Input_Result.reset_index(), on = 'symbol', how = 'left').set_index('symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c8279-53b8-4c22-a400-7246ee3bcfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d633480-ad55-46b3-81d4-87d523f14459",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparison_single_global_models['Train'] = Comparison_single_global_models['Acc_Train'] / Comparison_single_global_models['Acc_Train_global']\n",
    "Comparison_single_global_models['Test']  = Comparison_single_global_models['Acc_Test']  / Comparison_single_global_models['Acc_Test_global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d774a5-1f1c-4eb5-a14c-4f41a6e1bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparison_single_global_models[['Train','Test']].sort_values('Test',ascending = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b64179-95a8-4d4a-8479-da0e4a84ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "\n",
    "dates_test = X_test.index.unique()\n",
    "batches = 5\n",
    "X_train_new = X_train\n",
    "y_train_new = y_train\n",
    "y_preds_total = []\n",
    "y_test_total = pd.DataFrame()\n",
    "\n",
    "for i in range(int(np.floor(X_test.index.unique().shape[0] / batches))):\n",
    "    \n",
    "    print(i/int(np.floor(X_test.index.unique().shape[0] / batches)))\n",
    "    train_new_date      = dates_test[(i)*batches : (i+1)*batches]\n",
    "    X_train_to_be_added = X_test[X_test.index.isin(train_new_date)]\n",
    "    y_train_to_be_added = y_test[y_test.index.isin(train_new_date)]\n",
    "    X_train_new         = pd.concat([X_train_new, X_train_to_be_added])\n",
    "    y_train_new         = pd.concat([y_train_new, y_train_to_be_added])\n",
    "\n",
    "    y_train_new['weight1'] = 1\n",
    "    y_train_new['weight2'] = 1\n",
    "    y_train_new['weight'] = 1\n",
    "    count = 0\n",
    "    decay_rate = 0.002\n",
    "    for j in y_train_new.sort_index(ascending = True).index.unique().tolist():\n",
    "        y_train_new.loc[y_train_new.index == j, 'weight1'] = 1 - np.exp(-decay_rate * count)/2\n",
    "        count = count + 1\n",
    "        \n",
    "    y_train_new.loc[(np.abs(y_train_new['ret_abs'])>0.005) & (np.abs(y_train_new['ret_abs'])<0.02), 'weight2'] = 1.25\n",
    "    y_train_new.loc[np.abs(y_train_new['ret_abs'])>0.02, 'weight2'] = 1.5\n",
    "    \n",
    "    y_train_new['weight'] = y_train_new['weight1'] * y_train_new['weight2']\n",
    "    weights_input = y_train_new['weight'].tolist()\n",
    "    \n",
    "    test_new_date       = dates_test[(i+1)*batches : (i+2)*batches]\n",
    "    X_test_new          = X_test[X_test.index.isin(test_new_date)]\n",
    "    y_test_new          = y_test[y_test.index.isin(test_new_date)]\n",
    "    y_test_total        = pd.concat([y_test_total,y_test_new])\n",
    "    \n",
    "    y_pred, _ , _ , _ , _ = XGB(X_train_new.iloc[:,1:], X_test_new.iloc[:,1:], y_train_new['label_four'], y_test_new['label_four'], is_binary = False, Weight_input = weights_input, mode = 'regression', num_boost=50, \n",
    "                                                                                 alpha=0, lambda_=0, show_result = False)\n",
    "    \n",
    "    y_preds_total.extend(y_pred)\n",
    "\n",
    "class_labels = ['Class 1'] \n",
    "y_proba_df = pd.DataFrame(y_preds_total, columns=class_labels, index=y_test_total.index)\n",
    "y_proba_df['return'] = y_test_total['ret_abs']\n",
    "compare_df, y_proba_df = Analyze_output(y_proba_df, y_test_total, label_type = 'Binary', fine_tune = False, remove_ques = True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4c3a5-3d03-4de4-9d13-0958dad88078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "# y_pred_lstm_binary, y_proba_lstm_binary, model_lstm_binary = LSTM_model(LSTM_model(combined_X_train, combined_X_test, combined_y_train, combined_y_test, is_binary = True, epoch = 5)\n",
    "y_pred_lstm_multi,  y_proba_lstm_multi,  model_lstm_multi, X_test_ts, y_test_ts = LSTM_model(X_train_ts, X_test_ts, y_train_ts, y_test_ts, is_binary = False, label = 'label_four', epoch = 5)\n",
    "model_lstm_multi.save('global_lstm_multi_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a5e22-aca9-4490-8ced-2cb0c794301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "y_pred_rf_binary, y_proba_rf_binary, importance_rf_binary, rf_model_binary = Random_Forest(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'])\n",
    "y_pred_rf_multi,  y_proba_rf_multi,  importance_rf_multi,  rf_model_multi  = Random_Forest(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four'], y_test['label_four'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9860d-c48d-41e2-bf5a-3d097988dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Matrix\n",
    "y_pred_svm_binary, y_proba_svm_binary, importance_svm_binary, svm_model_binary = Support_Vector_Machine(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'])\n",
    "# y_pred_svm_multi,  y_proba_svm_multi,  importance_svm_multi,  svm_model_multi  = Support_Vector_Machine(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four'], y_test['label_four'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07894d4-749d-4174-a283-337459ca06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Networkt\n",
    "# y_pred_ann_binary, y_proba_ann_binary, importance_ann_binary, ann_model_binary = Artificial_Neural_Networkt(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label'], y_test['label'], is_binary=True)\n",
    "# y_pred_ann_multi,  y_proba_ann_multi,  importance_ann_multi,  ann_model_multi  = Artificial_Neural_Networkt(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four'], y_test['label_four'], is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95396704-cd55-4734-af18-8cb089db469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "y_pred_transformer_multi,  y_proba_transformer_multi,  model_transformer_multi, X_test_ts, y_test_ts = Transformer_model(X_train_ts, X_test_ts, y_train_ts, y_test_ts, \n",
    "                                                                                                                         lags = 15, is_binary = False, label = 'label_four', epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2520f81-7db3-4e24-b834-79a4d0710468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "y_pred_bgg_multi,    y_proba_bgg_multi,   importance_bgg_multi,   bgg_model_multi   = Bagging(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four'] - 1, y_test['label_four'] - 1, \n",
    "                                                                                              is_binary = False, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474900a8-10e3-4938-b3bb-1cf14b272138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Decent\n",
    "y_pred_sgd_multi,    y_proba_sgd_multi,   importance_sgd_multi,   sgd_model_multi = SGD_Classifier(X_train.iloc[:,1:], X_test.iloc[:,1:], y_train['label_four'] - 1, y_test['label_four'] - 1, \n",
    "                                                                                                   max_iter=1000, tol=1e-3, alpha=0.0001, loss='log_loss', learning_rate='optimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc78689-81e4-4208-b5bf-f955b668437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Output for Analyzing\n",
    "\n",
    "mode = 'Binary'        # Options = 'Binary', 'Multi', 'Regression'\n",
    "model = 'xgb'          # models list = 'svm', 'xgb', 'dt', 'rf', 'ann', 'lstm','transformer','bagging', 'sgd'\n",
    "relative = False\n",
    "\n",
    "if mode == 'Binary':\n",
    "    \n",
    "    y_proba_df = pd.DataFrame(y_proba_xgb_binary, columns=['Class 1'], index=y_test.index)\n",
    "    compare_df, y_proba_df = Analyze_output(y_proba_df = y_proba_df, y_test = y_test, remove_ques = False, pred_is_binary = True, actual_is_binary = False, pred_label_based_on = 'Class 1', \n",
    "                                            actual_label_based_on = 'label_multi_classes_10_days', return_based_on = 'ret_abs_10_days')\n",
    "\n",
    "if mode == 'Regression':\n",
    "    class_labels = ['Class 1']         \n",
    "    y_proba_df = pd.DataFrame(y_test_xgb_regrs, columns=class_labels, index=y_test.index) \n",
    "    \n",
    "    compare_df, y_proba_df = Analyze_output(y_proba_df = y_proba_df, y_test = y_test, remove_ques = False, pred_is_binary = True, actual_is_binary = False, pred_label_based_on = 'Class 1', \n",
    "                                            actual_label_based_on = 'label_multi_classes_20_days', return_based_on = 'ret_abs_20_days')\n",
    "\n",
    "\n",
    "if mode == 'Multi':\n",
    "    y_proba_df = pd.DataFrame(y_pred_xgb_multi, columns=['Class 1'], index=y_test.index) \n",
    "    compare_df, y_proba_df = Analyze_output(y_proba_df = y_proba_df, y_test = y_test, remove_ques = True, pred_is_binary = False, actual_is_binary = False, pred_label_based_on = 'Class 1', \n",
    "                                            actual_label_based_on = 'label_multi_classes_20_days', return_based_on = 'ret_abs_20_days')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c97be0-c28f-43da-8178-da29ceae60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the resukt\n",
    "show_output(compare_df, y_proba_df, confusion_matrix = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67381d11-9177-49c6-807f-77527d1b07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_df['period_ending_date']  = y_test['period_ending_date'] \n",
    "dates = []\n",
    "for i in y_proba_df['period_ending_date'].unique():\n",
    "    if y_proba_df[y_proba_df['period_ending_date'] == i].shape[0] > 50:\n",
    "        dates.append(i)\n",
    "\n",
    "y_proba_df = y_proba_df[y_proba_df['period_ending_date'].isin(dates)]   \n",
    "\n",
    "# if actual_is_binary: \n",
    "#         quantiles_by_date_Ret = y_proba_df.groupby(y_proba_df['period_ending_date'])[actual_label_based_on].quantile([0, 0.25, 0.5, 0.75, 1]).unstack()\n",
    "#         y_proba_df['label_predicted'] = y_proba_df.apply(lambda row: assign_label(row, quantiles_by_date_Ret, labels, col = 'Class 1'), axis=1)\n",
    "\n",
    "\n",
    "remove_ques = False\n",
    "\n",
    "very_highs, highs, lows, very_lows  = [], [], [], []\n",
    "very_highs_num, highs_num, lows_num, very_lows_num = [], [], [], [] \n",
    "compare_high_low, Absolute_Error = [], []\n",
    "return_based_on = 'ret_abs_10_days'\n",
    "        \n",
    "for i in dates:\n",
    "\n",
    "    if remove_ques:\n",
    "        very_high  = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 4) & (y_proba_df['que'] != 1)][return_based_on].mean()\n",
    "        high       = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 3) & (y_proba_df['que'] != 1)][return_based_on].mean()\n",
    "        low        = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 2) & (y_proba_df['que'] != -1)][return_based_on].mean()\n",
    "        very_low   = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 1) & (y_proba_df['que'] != -1)][return_based_on].mean()\n",
    "\n",
    "        very_high_num = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 4) & (y_proba_df['que'] != 1)].shape[0]\n",
    "        high_num      = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 3) & (y_proba_df['que'] != 1)].shape[0]\n",
    "        low_num       = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 2) & (y_proba_df['que'] != -1)].shape[0]\n",
    "        very_low_num  = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 1) & (y_proba_df['que'] != -1)].shape[0]\n",
    "        \n",
    "    else:            \n",
    "        very_high  = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 4)][return_based_on].mean()\n",
    "        high       = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 3)][return_based_on].mean()\n",
    "        low        = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 2)][return_based_on].mean()\n",
    "        very_low   = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 1)][return_based_on].mean()\n",
    "\n",
    "        very_high_num = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 4)].shape[0]\n",
    "        high_num      = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 3)].shape[0]\n",
    "        low_num       = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 2)].shape[0]\n",
    "        very_low_num  = y_proba_df[(y_proba_df['period_ending_date'] == i) & (y_proba_df['label_predicted'] == 1)].shape[0]\n",
    "\n",
    "    absolute_error = y_proba_df.loc[y_proba_df['period_ending_date'] == i, 'error'].sum() / y_proba_df.loc[y_proba_df['period_ending_date'] == i, 'error'].shape[0]\n",
    "\n",
    "    comparing = 0\n",
    "    if very_high > very_low:\n",
    "        comparing = comparing + 1\n",
    "    if high > low:\n",
    "        comparing = comparing + 1\n",
    "    if low > very_low:\n",
    "        comparing = comparing + 1        \n",
    "    compare_high_low.append(comparing/3)\n",
    "\n",
    "    \n",
    "    very_highs.append(very_high)\n",
    "    highs.append(high)\n",
    "    lows.append(low)\n",
    "    very_lows.append(very_low)\n",
    "    very_highs_num.append(very_high_num)\n",
    "    highs_num.append(high_num)\n",
    "    lows_num.append(low_num)\n",
    "    very_lows_num.append(very_low_num)\n",
    "    Absolute_Error.append(absolute_error)\n",
    "        \n",
    "compare_df = pd.DataFrame({'Very_Highs': very_highs, 'Highs': highs, 'Lows': lows, 'Very_Lows': very_lows, \n",
    "                            'Very_Highs_N': very_highs_num, 'Highs_N': highs_num, 'Lows_N': lows_num, 'Very_Lows_N': very_lows_num,\n",
    "                            'Compare': compare_high_low, 'Absolute_Error': Absolute_Error}, index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3c995-bde6-403f-a3b6-425bb771042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Report Publish Pattern\n",
    "\n",
    "# income_statement = pd.read_parquet('incomestatement.parquet')\n",
    "# income_statement = income_statement.reset_index().set_index('period_ending_date')\n",
    "# income_statement.sort_index(inplace = True, ascending = True)\n",
    "# ending_dates = income_statement.index.unique().tolist()\n",
    "\n",
    "# for i in ending_dates[0:]:\n",
    "  \n",
    "#     stocks_available = income_statement.loc[income_statement.index == i,['symbol','publish_date']]    \n",
    "#     num_total = stocks_available.shape[0] \n",
    "#     if num_total > 150:\n",
    "#         print('')\n",
    "#         print(i)\n",
    "#         print('')\n",
    "#         for days in range(30,90):\n",
    "#             ratio = stocks_available[stocks_available['publish_date'] <= i + timedelta(days=days)].shape[0]/num_total\n",
    "#             print(str(i + timedelta(days=days)) + ' : ' + str(ratio))\n",
    "#             if ratio > 0.8:\n",
    "#                 break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
